{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17459958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow==11.3.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (11.3.0)\n",
      "Requirement already satisfied: torch==2.7.1 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.7.1)\n",
      "Requirement already satisfied: torchvision==0.22.1 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (0.22.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch==2.7.1->-r requirements.txt (line 2)) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch==2.7.1->-r requirements.txt (line 2)) (4.14.1)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch==2.7.1->-r requirements.txt (line 2)) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch==2.7.1->-r requirements.txt (line 2)) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch==2.7.1->-r requirements.txt (line 2)) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch==2.7.1->-r requirements.txt (line 2)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch==2.7.1->-r requirements.txt (line 2)) (2025.7.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch==2.7.1->-r requirements.txt (line 2)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch==2.7.1->-r requirements.txt (line 2)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.venv/lib/python3.12/site-packages (from torch==2.7.1->-r requirements.txt (line 2)) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./.venv/lib/python3.12/site-packages (from torch==2.7.1->-r requirements.txt (line 2)) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.venv/lib/python3.12/site-packages (from torch==2.7.1->-r requirements.txt (line 2)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.venv/lib/python3.12/site-packages (from torch==2.7.1->-r requirements.txt (line 2)) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.venv/lib/python3.12/site-packages (from torch==2.7.1->-r requirements.txt (line 2)) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.venv/lib/python3.12/site-packages (from torch==2.7.1->-r requirements.txt (line 2)) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.venv/lib/python3.12/site-packages (from torch==2.7.1->-r requirements.txt (line 2)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.venv/lib/python3.12/site-packages (from torch==2.7.1->-r requirements.txt (line 2)) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.venv/lib/python3.12/site-packages (from torch==2.7.1->-r requirements.txt (line 2)) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch==2.7.1->-r requirements.txt (line 2)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.venv/lib/python3.12/site-packages (from torch==2.7.1->-r requirements.txt (line 2)) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.venv/lib/python3.12/site-packages (from torch==2.7.1->-r requirements.txt (line 2)) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in ./.venv/lib/python3.12/site-packages (from torch==2.7.1->-r requirements.txt (line 2)) (3.3.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from torchvision==0.22.1->-r requirements.txt (line 3)) (2.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.1->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch==2.7.1->-r requirements.txt (line 2)) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbb3ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56490c8c",
   "metadata": {},
   "source": [
    "# Data Transform and Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e2dd993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79f585a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "\n",
    "transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize(size=(IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23f736a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(root=\"./data/seg_train/seg_train\",\n",
    "                            transform=transforms,\n",
    "                            loader=pil_loader)\n",
    "\n",
    "val_dataset = ImageFolder(root=\"./data/seg_test/seg_test\",\n",
    "                            transform=transforms,\n",
    "                            loader=pil_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a699a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=16,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=16,\n",
    "                        shuffle=False,\n",
    "                        num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f797ba",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15d4ae76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu): ReLU()\n",
       "  (fc1): Linear(in_features=6272, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from CNN import CNN\n",
    "\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "model = CNN(IMAGE_SIZE, num_conv_layers=5, num_classes=num_classes)\n",
    "\n",
    "# use gpu is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3839796",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dd86de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 25\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimiser,\n",
    "                                                       T_max=NUM_EPOCH*len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0c2c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)  # output shape: [batch size, 6]\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06cd488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)  # output shape: [batch size, 6]\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bca5709a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 1.0438, Accuracy = 0.7100\n",
      "Epoch 2: Training Loss = 0.7388, Accuracy = 0.7317\n",
      "Epoch 3: Training Loss = 0.6356, Accuracy = 0.7680\n",
      "Epoch 4: Training Loss = 0.5375, Accuracy = 0.7880\n",
      "Epoch 5: Training Loss = 0.4632, Accuracy = 0.8003\n",
      "Epoch 6: Training Loss = 0.3913, Accuracy = 0.8170\n",
      "Epoch 7: Training Loss = 0.3195, Accuracy = 0.8220\n",
      "Epoch 8: Training Loss = 0.2570, Accuracy = 0.8020\n",
      "Epoch 9: Training Loss = 0.1924, Accuracy = 0.8040\n",
      "Epoch 10: Training Loss = 0.1416, Accuracy = 0.7993\n",
      "Epoch 11: Training Loss = 0.1046, Accuracy = 0.8057\n",
      "Epoch 12: Training Loss = 0.0731, Accuracy = 0.8063\n",
      "Epoch 13: Training Loss = 0.0630, Accuracy = 0.8003\n",
      "Epoch 14: Training Loss = 0.0289, Accuracy = 0.7997\n",
      "Epoch 15: Training Loss = 0.0206, Accuracy = 0.8183\n",
      "Epoch 16: Training Loss = 0.0222, Accuracy = 0.8043\n",
      "Epoch 17: Training Loss = 0.0127, Accuracy = 0.8107\n",
      "Epoch 18: Training Loss = 0.0080, Accuracy = 0.8157\n",
      "Epoch 19: Training Loss = 0.0068, Accuracy = 0.8160\n",
      "Epoch 20: Training Loss = 0.0047, Accuracy = 0.8190\n",
      "Epoch 21: Training Loss = 0.0045, Accuracy = 0.8197\n",
      "Epoch 22: Training Loss = 0.0035, Accuracy = 0.8180\n",
      "Epoch 23: Training Loss = 0.0024, Accuracy = 0.8190\n",
      "Epoch 24: Training Loss = 0.0018, Accuracy = 0.8200\n",
      "Epoch 25: Training Loss = 0.0016, Accuracy = 0.8207\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCH):\n",
    "    train_loss = train(model, train_loader, optimiser, loss_fn, device)\n",
    "    val_acc = evaluate(model, val_loader, device)\n",
    "    print(f\"Epoch {epoch+1}: Training Loss = {train_loss:.4f}, Accuracy = {val_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
